{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d5772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "TITAN Xp\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "Using  4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using \", torch.cuda.device_count(), \"GPUs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7279acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2eee613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from loguru import logger\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.functional import resample\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from pathlib import Path\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "from core.soundstream import SoundStream\n",
    "from trainer.trainer import SoundStreamTrainer\n",
    "from datasets.data import SoundDataset, get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe3299c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d07083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55429326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    use_local_attn = True,\n",
    "    use_mhesa = True,\n",
    "    rq_num_quantizers = 8,\n",
    "    attn_window_size = 128,       # local attention receptive field at bottleneck\n",
    "    attn_depth = 2                # 2 local attention transformer blocks - the soundstream folks were not experts with attention, so i took the liberty to add some. encodec went with lstms, but attention should be better\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0e501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85b7a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/srv/share4/sanisetty3/MagnaTagATune/data\"\n",
    "num_outputs = 1\n",
    "max_length = 2*24000\n",
    "# max_length = cast_tuple(max_length, num_outputs)\n",
    "target_sample_hz = 24000\n",
    "# target_sample_hz = cast_tuple(target_sample_hz)\n",
    "seq_len_multiple_of = 480\n",
    "# seq_len_multiple_of = cast_tuple(seq_len_multiple_of, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a1c13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(f\"{folder}/**/*.mp3\" , recursive=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fc94e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sample_hz = torchaudio.load(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "376392a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 465984])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33225a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SoundDataset(\n",
    "            folder,\n",
    "            max_length = max_length,\n",
    "            target_sample_hz = target_sample_hz,\n",
    "            seq_len_multiple_of = seq_len_multiple_of\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19575a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = get_dataloader(ds, batch_size = 2, num_workers = 0, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94aa15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c31758c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundstream, dl = accelerator.prepare(\n",
    "    soundstream, dl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dee527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_iter = cycle(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9455fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave, = next(dl_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c6c8f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.conv.weight torch.float32\n",
      "encoder.0.conv.bias torch.float32\n",
      "encoder.1.0.fn.0.conv.weight torch.float32\n",
      "encoder.1.0.fn.0.conv.bias torch.float32\n",
      "encoder.1.0.fn.2.conv.weight torch.float32\n",
      "encoder.1.0.fn.2.conv.bias torch.float32\n",
      "encoder.1.1.fn.0.conv.weight torch.float32\n",
      "encoder.1.1.fn.0.conv.bias torch.float32\n",
      "encoder.1.1.fn.2.conv.weight torch.float32\n",
      "encoder.1.1.fn.2.conv.bias torch.float32\n",
      "encoder.1.2.fn.0.conv.weight torch.float32\n",
      "encoder.1.2.fn.0.conv.bias torch.float32\n",
      "encoder.1.2.fn.2.conv.weight torch.float32\n",
      "encoder.1.2.fn.2.conv.bias torch.float32\n",
      "encoder.1.3.conv.weight torch.float32\n",
      "encoder.1.3.conv.bias torch.float32\n",
      "encoder.2.0.fn.0.conv.weight torch.float32\n",
      "encoder.2.0.fn.0.conv.bias torch.float32\n",
      "encoder.2.0.fn.2.conv.weight torch.float32\n",
      "encoder.2.0.fn.2.conv.bias torch.float32\n",
      "encoder.2.1.fn.0.conv.weight torch.float32\n",
      "encoder.2.1.fn.0.conv.bias torch.float32\n",
      "encoder.2.1.fn.2.conv.weight torch.float32\n",
      "encoder.2.1.fn.2.conv.bias torch.float32\n",
      "encoder.2.2.fn.0.conv.weight torch.float32\n",
      "encoder.2.2.fn.0.conv.bias torch.float32\n",
      "encoder.2.2.fn.2.conv.weight torch.float32\n",
      "encoder.2.2.fn.2.conv.bias torch.float32\n",
      "encoder.2.3.conv.weight torch.float32\n",
      "encoder.2.3.conv.bias torch.float32\n",
      "encoder.3.0.fn.0.conv.weight torch.float32\n",
      "encoder.3.0.fn.0.conv.bias torch.float32\n",
      "encoder.3.0.fn.2.conv.weight torch.float32\n",
      "encoder.3.0.fn.2.conv.bias torch.float32\n",
      "encoder.3.1.fn.0.conv.weight torch.float32\n",
      "encoder.3.1.fn.0.conv.bias torch.float32\n",
      "encoder.3.1.fn.2.conv.weight torch.float32\n",
      "encoder.3.1.fn.2.conv.bias torch.float32\n",
      "encoder.3.2.fn.0.conv.weight torch.float32\n",
      "encoder.3.2.fn.0.conv.bias torch.float32\n",
      "encoder.3.2.fn.2.conv.weight torch.float32\n",
      "encoder.3.2.fn.2.conv.bias torch.float32\n",
      "encoder.3.3.conv.weight torch.float32\n",
      "encoder.3.3.conv.bias torch.float32\n",
      "encoder.4.0.fn.0.conv.weight torch.float32\n",
      "encoder.4.0.fn.0.conv.bias torch.float32\n",
      "encoder.4.0.fn.2.conv.weight torch.float32\n",
      "encoder.4.0.fn.2.conv.bias torch.float32\n",
      "encoder.4.1.fn.0.conv.weight torch.float32\n",
      "encoder.4.1.fn.0.conv.bias torch.float32\n",
      "encoder.4.1.fn.2.conv.weight torch.float32\n",
      "encoder.4.1.fn.2.conv.bias torch.float32\n",
      "encoder.4.2.fn.0.conv.weight torch.float32\n",
      "encoder.4.2.fn.0.conv.bias torch.float32\n",
      "encoder.4.2.fn.2.conv.weight torch.float32\n",
      "encoder.4.2.fn.2.conv.bias torch.float32\n",
      "encoder.4.3.conv.weight torch.float32\n",
      "encoder.4.3.conv.bias torch.float32\n",
      "encoder.5.conv.weight torch.float32\n",
      "encoder.5.conv.bias torch.float32\n",
      "encoder_attn.0.attn.q_scale torch.float32\n",
      "encoder_attn.0.attn.k_scale torch.float32\n",
      "encoder_attn.0.attn.norm.weight torch.float32\n",
      "encoder_attn.0.attn.norm.bias torch.float32\n",
      "encoder_attn.0.attn.to_qkv.weight torch.float32\n",
      "encoder_attn.0.attn.to_out.weight torch.float32\n",
      "encoder_attn.0.ff.0.weight torch.float32\n",
      "encoder_attn.0.ff.0.bias torch.float32\n",
      "encoder_attn.0.ff.1.weight torch.float32\n",
      "encoder_attn.0.ff.4.weight torch.float32\n",
      "encoder_attn.1.attn.q_scale torch.float32\n",
      "encoder_attn.1.attn.k_scale torch.float32\n",
      "encoder_attn.1.attn.norm.weight torch.float32\n",
      "encoder_attn.1.attn.norm.bias torch.float32\n",
      "encoder_attn.1.attn.to_qkv.weight torch.float32\n",
      "encoder_attn.1.attn.to_out.weight torch.float32\n",
      "encoder_attn.1.ff.0.weight torch.float32\n",
      "encoder_attn.1.ff.0.bias torch.float32\n",
      "encoder_attn.1.ff.1.weight torch.float32\n",
      "encoder_attn.1.ff.4.weight torch.float32\n",
      "decoder_attn.0.attn.q_scale torch.float32\n",
      "decoder_attn.0.attn.k_scale torch.float32\n",
      "decoder_attn.0.attn.norm.weight torch.float32\n",
      "decoder_attn.0.attn.norm.bias torch.float32\n",
      "decoder_attn.0.attn.to_qkv.weight torch.float32\n",
      "decoder_attn.0.attn.to_out.weight torch.float32\n",
      "decoder_attn.0.ff.0.weight torch.float32\n",
      "decoder_attn.0.ff.0.bias torch.float32\n",
      "decoder_attn.0.ff.1.weight torch.float32\n",
      "decoder_attn.0.ff.4.weight torch.float32\n",
      "decoder_attn.1.attn.q_scale torch.float32\n",
      "decoder_attn.1.attn.k_scale torch.float32\n",
      "decoder_attn.1.attn.norm.weight torch.float32\n",
      "decoder_attn.1.attn.norm.bias torch.float32\n",
      "decoder_attn.1.attn.to_qkv.weight torch.float32\n",
      "decoder_attn.1.attn.to_out.weight torch.float32\n",
      "decoder_attn.1.ff.0.weight torch.float32\n",
      "decoder_attn.1.ff.0.bias torch.float32\n",
      "decoder_attn.1.ff.1.weight torch.float32\n",
      "decoder_attn.1.ff.4.weight torch.float32\n",
      "decoder.0.conv.weight torch.float32\n",
      "decoder.0.conv.bias torch.float32\n",
      "decoder.1.0.conv.weight torch.float32\n",
      "decoder.1.0.conv.bias torch.float32\n",
      "decoder.1.1.fn.0.conv.weight torch.float32\n",
      "decoder.1.1.fn.0.conv.bias torch.float32\n",
      "decoder.1.1.fn.2.conv.weight torch.float32\n",
      "decoder.1.1.fn.2.conv.bias torch.float32\n",
      "decoder.1.2.fn.0.conv.weight torch.float32\n",
      "decoder.1.2.fn.0.conv.bias torch.float32\n",
      "decoder.1.2.fn.2.conv.weight torch.float32\n",
      "decoder.1.2.fn.2.conv.bias torch.float32\n",
      "decoder.1.3.fn.0.conv.weight torch.float32\n",
      "decoder.1.3.fn.0.conv.bias torch.float32\n",
      "decoder.1.3.fn.2.conv.weight torch.float32\n",
      "decoder.1.3.fn.2.conv.bias torch.float32\n",
      "decoder.2.0.conv.weight torch.float32\n",
      "decoder.2.0.conv.bias torch.float32\n",
      "decoder.2.1.fn.0.conv.weight torch.float32\n",
      "decoder.2.1.fn.0.conv.bias torch.float32\n",
      "decoder.2.1.fn.2.conv.weight torch.float32\n",
      "decoder.2.1.fn.2.conv.bias torch.float32\n",
      "decoder.2.2.fn.0.conv.weight torch.float32\n",
      "decoder.2.2.fn.0.conv.bias torch.float32\n",
      "decoder.2.2.fn.2.conv.weight torch.float32\n",
      "decoder.2.2.fn.2.conv.bias torch.float32\n",
      "decoder.2.3.fn.0.conv.weight torch.float32\n",
      "decoder.2.3.fn.0.conv.bias torch.float32\n",
      "decoder.2.3.fn.2.conv.weight torch.float32\n",
      "decoder.2.3.fn.2.conv.bias torch.float32\n",
      "decoder.3.0.conv.weight torch.float32\n",
      "decoder.3.0.conv.bias torch.float32\n",
      "decoder.3.1.fn.0.conv.weight torch.float32\n",
      "decoder.3.1.fn.0.conv.bias torch.float32\n",
      "decoder.3.1.fn.2.conv.weight torch.float32\n",
      "decoder.3.1.fn.2.conv.bias torch.float32\n",
      "decoder.3.2.fn.0.conv.weight torch.float32\n",
      "decoder.3.2.fn.0.conv.bias torch.float32\n",
      "decoder.3.2.fn.2.conv.weight torch.float32\n",
      "decoder.3.2.fn.2.conv.bias torch.float32\n",
      "decoder.3.3.fn.0.conv.weight torch.float32\n",
      "decoder.3.3.fn.0.conv.bias torch.float32\n",
      "decoder.3.3.fn.2.conv.weight torch.float32\n",
      "decoder.3.3.fn.2.conv.bias torch.float32\n",
      "decoder.4.0.conv.weight torch.float32\n",
      "decoder.4.0.conv.bias torch.float32\n",
      "decoder.4.1.fn.0.conv.weight torch.float32\n",
      "decoder.4.1.fn.0.conv.bias torch.float32\n",
      "decoder.4.1.fn.2.conv.weight torch.float32\n",
      "decoder.4.1.fn.2.conv.bias torch.float32\n",
      "decoder.4.2.fn.0.conv.weight torch.float32\n",
      "decoder.4.2.fn.0.conv.bias torch.float32\n",
      "decoder.4.2.fn.2.conv.weight torch.float32\n",
      "decoder.4.2.fn.2.conv.bias torch.float32\n",
      "decoder.4.3.fn.0.conv.weight torch.float32\n",
      "decoder.4.3.fn.0.conv.bias torch.float32\n",
      "decoder.4.3.fn.2.conv.weight torch.float32\n",
      "decoder.4.3.fn.2.conv.bias torch.float32\n",
      "decoder.5.conv.weight torch.float32\n",
      "decoder.5.conv.bias torch.float32\n",
      "discriminators.0.init_conv.weight torch.float32\n",
      "discriminators.0.init_conv.bias torch.float32\n",
      "discriminators.0.conv_layers.0.0.weight torch.float32\n",
      "discriminators.0.conv_layers.0.0.bias torch.float32\n",
      "discriminators.0.conv_layers.1.0.weight torch.float32\n",
      "discriminators.0.conv_layers.1.0.bias torch.float32\n",
      "discriminators.0.conv_layers.2.0.weight torch.float32\n",
      "discriminators.0.conv_layers.2.0.bias torch.float32\n",
      "discriminators.0.conv_layers.3.0.weight torch.float32\n",
      "discriminators.0.conv_layers.3.0.bias torch.float32\n",
      "discriminators.0.final_conv.0.weight torch.float32\n",
      "discriminators.0.final_conv.0.bias torch.float32\n",
      "discriminators.0.final_conv.2.weight torch.float32\n",
      "discriminators.0.final_conv.2.bias torch.float32\n",
      "discriminators.1.init_conv.weight torch.float32\n",
      "discriminators.1.init_conv.bias torch.float32\n",
      "discriminators.1.conv_layers.0.0.weight torch.float32\n",
      "discriminators.1.conv_layers.0.0.bias torch.float32\n",
      "discriminators.1.conv_layers.1.0.weight torch.float32\n",
      "discriminators.1.conv_layers.1.0.bias torch.float32\n",
      "discriminators.1.conv_layers.2.0.weight torch.float32\n",
      "discriminators.1.conv_layers.2.0.bias torch.float32\n",
      "discriminators.1.conv_layers.3.0.weight torch.float32\n",
      "discriminators.1.conv_layers.3.0.bias torch.float32\n",
      "discriminators.1.final_conv.0.weight torch.float32\n",
      "discriminators.1.final_conv.0.bias torch.float32\n",
      "discriminators.1.final_conv.2.weight torch.float32\n",
      "discriminators.1.final_conv.2.bias torch.float32\n",
      "discriminators.2.init_conv.weight torch.float32\n",
      "discriminators.2.init_conv.bias torch.float32\n",
      "discriminators.2.conv_layers.0.0.weight torch.float32\n",
      "discriminators.2.conv_layers.0.0.bias torch.float32\n",
      "discriminators.2.conv_layers.1.0.weight torch.float32\n",
      "discriminators.2.conv_layers.1.0.bias torch.float32\n",
      "discriminators.2.conv_layers.2.0.weight torch.float32\n",
      "discriminators.2.conv_layers.2.0.bias torch.float32\n",
      "discriminators.2.conv_layers.3.0.weight torch.float32\n",
      "discriminators.2.conv_layers.3.0.bias torch.float32\n",
      "discriminators.2.final_conv.0.weight torch.float32\n",
      "discriminators.2.final_conv.0.bias torch.float32\n",
      "discriminators.2.final_conv.2.weight torch.float32\n",
      "discriminators.2.final_conv.2.bias torch.float32\n",
      "stft_discriminator.init_conv.weight torch.float32\n",
      "stft_discriminator.init_conv.bias torch.float32\n",
      "stft_discriminator.layers.0.0.weight torch.float32\n",
      "stft_discriminator.layers.0.0.bias torch.float32\n",
      "stft_discriminator.layers.0.1.b torch.float32\n",
      "stft_discriminator.layers.0.2.weight torch.float32\n",
      "stft_discriminator.layers.0.2.bias torch.float32\n",
      "stft_discriminator.layers.1.0.weight torch.float32\n",
      "stft_discriminator.layers.1.0.bias torch.float32\n",
      "stft_discriminator.layers.1.1.b torch.float32\n",
      "stft_discriminator.layers.1.2.weight torch.float32\n",
      "stft_discriminator.layers.1.2.bias torch.float32\n",
      "stft_discriminator.layers.2.0.weight torch.float32\n",
      "stft_discriminator.layers.2.0.bias torch.float32\n",
      "stft_discriminator.layers.2.1.b torch.float32\n",
      "stft_discriminator.layers.2.2.weight torch.float32\n",
      "stft_discriminator.layers.2.2.bias torch.float32\n",
      "stft_discriminator.layers.3.0.weight torch.float32\n",
      "stft_discriminator.layers.3.0.bias torch.float32\n",
      "stft_discriminator.layers.3.1.b torch.float32\n",
      "stft_discriminator.layers.3.2.weight torch.float32\n",
      "stft_discriminator.layers.3.2.bias torch.float32\n",
      "stft_discriminator.layers.4.0.weight torch.float32\n",
      "stft_discriminator.layers.4.0.bias torch.float32\n",
      "stft_discriminator.layers.4.1.b torch.float32\n",
      "stft_discriminator.layers.4.2.weight torch.float32\n",
      "stft_discriminator.layers.4.2.bias torch.float32\n",
      "stft_discriminator.layers.5.0.weight torch.float32\n",
      "stft_discriminator.layers.5.0.bias torch.float32\n",
      "stft_discriminator.layers.5.1.b torch.float32\n",
      "stft_discriminator.layers.5.2.weight torch.float32\n",
      "stft_discriminator.layers.5.2.bias torch.float32\n",
      "stft_discriminator.final_conv.weight torch.float32\n",
      "stft_discriminator.final_conv.bias torch.float32\n"
     ]
    }
   ],
   "source": [
    "for nme,param in soundstream.named_parameters():\n",
    "    print(nme , param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc9b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fcbedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c335a2fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuFFT only supports dimensions whose sizes are powers of two when computing in half precision, but got a signal size of[48000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c71f903bd6e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# with autocast():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_spectral_recon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_commitment_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoundstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_loss_breakdown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'@autocast() decorator is not supported in script mode'\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/soundstream_music/core/soundstream.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_encoded, return_discr_loss, return_discr_losses_separately, return_loss_breakdown, return_recons_only, input_sample_hz, apply_grad_penalty)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0morig_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b c n -> b n c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/soundstream_music/core/soundstream.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b c n -> b n c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprenorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmhema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b n c -> b c n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/mega_pytorch/mega_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconv1d_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_learned_ema_with_damping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdampen_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/mega_pytorch/mega_pytorch.py\u001b[0m in \u001b[0;36mapply_learned_ema_with_damping\u001b[0;34m(x, alphas, dampen_factors)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# conv1d fft O(nlog(n))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconv1d_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_learned_ema_with_damping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdampen_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/mega_pytorch/mega_pytorch.py\u001b[0m in \u001b[0;36mconv1d_fft\u001b[0;34m(x, weights, dim, weight_dim)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_fast_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mM\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mf_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mf_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuFFT only supports dimensions whose sizes are powers of two when computing in half precision, but got a signal size of[48000]"
     ]
    }
   ],
   "source": [
    "# with autocast():\n",
    "loss, (recon_loss, multi_spectral_recon_loss, adversarial_loss, feature_loss, all_commitment_loss) = soundstream(wave, return_loss_breakdown = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d890afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54adc9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960949ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82770e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b712cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4093e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b605f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e944a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81d4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47622321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ead194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_dim = 768,\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    "    attn_window_size = 128,       # local attention receptive field at bottleneck\n",
    "    attn_depth = 2                # 2 local attention transformer blocks - the soundstream folks were not experts with attention, so i took the liberty to add some. encodec went with lstms, but attention should be better\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SoundStreamTrainer(\n",
    "    soundstream,\n",
    "    folder = \"/srv/share4/sanisetty3/MagnaTagATune/data\",\n",
    "    batch_size = 24,\n",
    "    grad_accum_every = 2,         # effective batch size of 32\n",
    "    data_max_length_seconds = 2,  # train on 2 second audio\n",
    "    num_train_steps = 10000,\n",
    "    results_folder = \"./checkpoints/no_deepspeed/fixed_input_length/\",\n",
    "    \n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4e818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7abb08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
